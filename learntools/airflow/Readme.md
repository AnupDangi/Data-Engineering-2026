# Apache Airflow: Orchestration for Data Engineers

Airflow is a platform that lets you build and run workflows. A workflow is represented as a DAG (a Directed Acyclic Graph), and contains individual pieces of work called Tasks, arranged with dependencies and data flows taken into account.

---

## ðŸ§  How Airflow Works

Apache Airflow reduces to one core idea:

> **Describe workflows as code, run them on a schedule, and never lose visibility.**

### The Mental Model

Think of Airflow as a **smart traffic controller** for your data pipelines:

1. **You define** â†’ What tasks need to run and in what order (DAG)
2. **Scheduler decides** â†’ When tasks should execute based on time/dependencies
3. **Executor runs** â†’ Tasks on available workers (local or distributed)
4. **Metadata DB tracks** â†’ Every execution, every retry, every failure
5. **UI shows** â†’ Real-time visibility into pipeline health

### Architecture Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AIRFLOW SYSTEM                        â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚  DAG Files  â”‚  â† You write these (Python code)       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚         â”‚                                                â”‚
â”‚         â†“                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  Scheduler  â”‚ â†â”€â”€â”€â”€â”€â†’ â”‚ Metadata DB â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚         â”‚                       â†‘                        â”‚
â”‚         â”‚                       â”‚                        â”‚
â”‚         â†“                       â”‚                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚                        â”‚
â”‚  â”‚   Executor  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚         â”‚                                                â”‚
â”‚         â†“                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚         Workers              â”‚                        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”‚                        â”‚
â”‚  â”‚  â”‚Task 1â”‚ â”‚Task 2â”‚ â”‚Task 3â”‚ â”‚                        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚  Webserver (UI: port 8080)  â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         External Systems (via Hooks)
              â”‚
              â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Database â”‚  Kafka   â”‚  Spark   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Component Flow**:

1. **DAG Files** â†’ Define workflows
2. **Scheduler** â†’ Parses DAGs, creates task instances, writes to DB
3. **Metadata DB** â†’ Central state store (PostgreSQL/MySQL)
4. **Executor** â†’ Queues tasks for execution
5. **Workers** â†’ Execute actual task logic
6. **Webserver** â†’ Provides UI for monitoring/triggering

### Execution Flow

**Step 1: DAG Definition**

```python
# You write this in ~/airflow/dags/
with DAG('my_pipeline', schedule_interval='@daily') as dag:
    task_a >> task_b >> task_c  # Define dependencies
```

**Step 2: Scheduler Parses**

- Scheduler reads your DAG file
- Checks if it's time to run (based on `schedule_interval`)
- Creates `DagRun` (one execution instance)
- Creates `TaskInstance` for each task

**Step 3: Executor Queues**

- Executor picks up task instances that are ready
- Checks dependencies (did upstream tasks succeed?)
- Queues task for execution

**Step 4: Worker Executes**

- Worker receives task
- Runs the actual logic (Python function, Bash command, etc.)
- Returns result: success/failure/retry

**Step 5: Metadata Updates**

- Every state change logged to database
- UI reflects real-time status
- XCom stores inter-task data

**Step 6: Scheduler Continues**

- On success â†’ trigger downstream tasks
- On failure â†’ retry (if configured) or mark failed
- On completion â†’ close DagRun

---

## ðŸ“– Essential Jargon (No Mysticism)

### 1. DAG (Directed Acyclic Graph)

**What**: The blueprint of your workflow.

**Decoded**:

- **Directed** â†’ Tasks have order (`A â†’ B`, not random)
- **Acyclic** â†’ No infinite loops (can't go `A â†’ B â†’ A`)
- **Graph** â†’ Visual representation of dependencies

**Key Point**: A DAG is _code_, not execution. It defines _what should happen_, not _when it happened_.

**Example**:

```python
# This IS a DAG
fetch_data >> clean_data >> save_data

# This is NOT allowed (cycle)
task_a >> task_b >> task_a  âŒ
```

**Visual Example**: Complex DAG with branching and trigger rules

![Airflow DAG with Trigger](https://airflow.apache.org/docs/apache-airflow/2.5.2/_images/branch_with_trigger.png)

_This shows how tasks can branch and converge based on conditions - a real workflow pattern you'll use._

---

### 2. Task

**What**: A single unit of work.

**Rules**:

- Does **one thing** (fetch data, NOT fetch + clean + save)
- Atomic (succeeds or fails completely)
- Idempotent (running twice = same result)

**Example**:

```python
fetch_user = PythonOperator(
    task_id='fetch_user',
    python_callable=fetch_user_function
)
```

**Task Dependencies Visual**:

```
Simple Linear:     task_a >> task_b >> task_c

Parallel:          task_a >> [task_b, task_c] >> task_d

Branching:         task_a >> branch_task
                           â”œâ”€> task_b >> join
                           â””â”€> task_c >> join
```

**Task Dependencies Visual**:

```
Simple Linear:     task_a >> task_b >> task_c

Parallel:          task_a >> [task_b, task_c] >> task_d

Branching:         task_a >> branch_task
                           â”œâ”€> task_b >> join
                           â””â”€> task_c >> join
```

---

### 3. Operator

**What**: A template that defines _how_ a task executes.

**Key Operators**:

| Operator               | Purpose             |
| ---------------------- | ------------------- |
| `PythonOperator`       | Run Python function |
| `BashOperator`         | Run shell command   |
| `EmailOperator`        | Send email          |
| `HttpOperator`         | Make API call       |
| `SparkSubmitOperator`  | Submit Spark job    |
| `KafkaProduceOperator` | Send to Kafka topic |

**Important**: Operator â‰  Task

- Operator = **class/template**
- Task = **instance of operator**

---

### 4. Executor

**What**: Determines **where and how** tasks run.

**Types**:

| Executor             | Use Case                         |
| -------------------- | -------------------------------- |
| `SequentialExecutor` | One task at a time (default/dev) |
| `LocalExecutor`      | Parallel on one machine          |
| `CeleryExecutor`     | Distributed across many workers  |
| `KubernetesExecutor` | Each task = Kubernetes pod       |

**Key Point**: Executor is **infrastructure**, not DAG logic. You change executor in `airflow.cfg`, not in your DAG code.

---

### 5. Scheduler

**What**: The brain that decides **when** DAGs run.

**Responsibilities**:

- Parse DAG files (every N seconds)
- Check if it's time to run (based on `schedule_interval`)
- Create DagRun instances
- Monitor task dependencies
- Handle retries
- Trigger downstream tasks

**Cron vs Airflow Scheduler**:

```
Cron:     "Run at 9 AM" â†’ fires, doesn't care if previous run failed
Airflow:  "Run at 9 AM" â†’ checks dependencies, retries, backfills, logs everything
```

---

### 6. DagRun

**What**: One execution instance of a DAG.

**Example**:

- DAG: `daily_sales_pipeline`
- DagRun 1: 2026-01-20 run
- DagRun 2: 2026-01-21 run

Each DagRun has:

- `execution_date` (logical timestamp)
- `state` (running/success/failed)
- `run_id` (unique identifier)

---

### 7. TaskInstance

**What**: One execution instance of a task within a DagRun.

**States**:

- `queued` â†’ waiting for executor
- `running` â†’ currently executing
- `success` â†’ completed successfully
- `failed` â†’ error occurred
- `up_for_retry` â†’ will retry
- `skipped` â†’ dependency failed

**State Transition Flow**:

```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  queued  â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ running  â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â†“             â†“              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ success â”‚   â”‚  failed  â”‚   â”‚skipped â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚up_for_retry  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
                   (back to queued)
```

---

### 8. XCom (Cross-Communication)

**What**: A mechanism to pass small data between tasks.

**Usage**:

```python
# Task 1: Push data
def fetch_user(**context):
    data = {"name": "John"}
    context["ti"].xcom_push(key="user_data", value=data)

# Task 2: Pull data
def process_user(**context):
    data = context["ti"].xcom_pull(
        key="user_data",
        task_ids="fetch_user"
    )
```

**Visual Flow**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       XCom        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Task A   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Metadata DB  â”‚
â”‚ (push)    â”‚  {key: "result"}  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â”‚ pull
                                        â†“
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚   Task B     â”‚
                                 â”‚  (pull)      â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Important**: XCom is for **metadata**, not large datasets.

- âœ… File path, record count, status flag
- âŒ 10GB DataFrame

---

### 9. Sensor

**What**: A special operator that **waits** for something to happen.

**Common Sensors**:

- `FileSensor` â†’ Wait for file to exist
- `HttpSensor` â†’ Wait for API to be healthy
- `TimeDeltaSensor` â†’ Wait for time duration
- `ExternalTaskSensor` â†’ Wait for another DAG

**Example**:

```python
wait_for_file = FileSensor(
    task_id='wait_for_data',
    filepath='/data/input.csv',
    poke_interval=30,  # Check every 30 seconds
    timeout=3600       # Give up after 1 hour
)
```

**How Sensors Work**:

```
Sensor Task Lifecycle:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”   Check    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚Start â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Exists? â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚
â”‚                           â”‚                 â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€Noâ”€â”€â”´â”€â”€â”€â”€Yesâ”€â”€â”€â”      â”‚
â”‚               â”‚                      â”‚      â”‚
â”‚               â†“                      â†“      â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚         â”‚  Wait    â”‚           â”‚ Success â”‚ â”‚
â”‚         â”‚30 sec... â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚              â”‚                              â”‚
â”‚              â”‚ (loop back)                  â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                          â”‚                  â”‚
â”‚                Timeout?  â†“                  â”‚
â”‚              Yes â”€â”€â”€> â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚                       â”‚ Failed â”‚            â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 10. Hook

**What**: A reusable interface to external systems.

**Purpose**: Abstracts connection logic (credentials, retries, etc.)

**Example**:

```python
from airflow.providers.postgres.hooks.postgres import PostgresHook

pg_hook = PostgresHook(postgres_conn_id='my_postgres')
records = pg_hook.get_records("SELECT * FROM users")
```

**Connection stored in Airflow UI** â†’ Security best practice.

---

### 11. Connection

**What**: Stored credentials for external systems.

**Stored in**: Airflow metadata DB or secrets backend

**Created via**: UI or CLI

```bash
airflow connections add my_postgres \
  --conn-type postgres \
  --conn-host localhost \
  --conn-login user \
  --conn-password pass
```

---

### 12. Variables

**What**: Key-value store for dynamic configuration.

**Use Cases**:

- API keys
- Environment flags
- Thresholds

**Example**:

```python
from airflow.models import Variable

batch_size = Variable.get("batch_size", default_var=100)
```

---

### 13. Backfill

**What**: Run DAG for past date ranges.

**Command**:

```bash
airflow dags backfill \
  --start-date 2026-01-01 \
  --end-date 2026-01-15 \
  my_pipeline
```

**Use Case**: You deployed a new pipeline, but need historical data processed.

---

### 14. Catchup

**What**: Config that determines if Airflow runs missed schedules.

**Example**:

```python
with DAG(
    dag_id='my_dag',
    start_date=datetime(2026, 1, 1),
    schedule_interval='@daily',
    catchup=False  # Don't run all missed dates since Jan 1
):
```

- `catchup=True` â†’ Backfill automatically
- `catchup=False` â†’ Only run from now onwards

---

### 15. Trigger Rule

**What**: Determines when a task should run based on upstream task states.

**Options**:

- `all_success` (default) â†’ All upstream tasks succeeded
- `all_failed` â†’ All upstream tasks failed
- `one_success` â†’ At least one upstream succeeded
- `one_failed` â†’ At least one upstream failed
- `none_failed` â†’ No upstream tasks failed (skipped OK)

**Example**:

```python
cleanup = PythonOperator(
    task_id='cleanup',
    python_callable=cleanup_function,
    trigger_rule='all_done'  # Run regardless of success/failure
)
```

**Visual Comparison**:

**With Trigger (all_done)**:
![Branch with Trigger](https://airflow.apache.org/docs/apache-airflow/2.5.2/_images/branch_with_trigger.png)

**Without Trigger (default all_success)**:
![Branch without Trigger](https://airflow.apache.org/docs/apache-airflow/2.6.0/_images/branch_without_trigger.png)

_Notice how trigger rules affect which downstream tasks execute when branches have different outcomes._

---

### 16. Pool

**What**: Limit concurrent task execution.

**Use Case**: Prevent overloading external systems.

**Example**:

```bash
# Create pool with max 5 concurrent tasks
airflow pools set postgres_pool 5 "PostgreSQL connections"
```

```python
query_task = PythonOperator(
    task_id='query_db',
    python_callable=query_function,
    pool='postgres_pool'  # Max 5 of these tasks run concurrently
)
```

---

### 17. SLA (Service Level Agreement)

**What**: Expected task completion time.

**Example**:

```python
fetch_data = PythonOperator(
    task_id='fetch_data',
    python_callable=fetch_function,
    sla=timedelta(minutes=30)  # Should complete in 30 min
)
```

If SLA missed â†’ Alert triggered.

---

## ðŸ§© How Airflow Fits Your Data Stack

| Tool                | Role                       |
| ------------------- | -------------------------- |
| **Kafka**           | Real-time event transport  |
| **Spark Streaming** | Continuous data processing |
| **Airflow**         | Orchestration & scheduling |

**Airflow coordinates**:

- Triggering Spark batch jobs
- Validating Bronze â†’ Silver â†’ Gold
- Scheduling ML training
- Monitoring pipeline health
- Backfilling historical data

**Visual: Complete Data Pipeline**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PIPELINE ECOSYSTEM                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  REAL-TIME LAYER
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Kafka   â”‚â”€â”€â”€â”€â”€â”€â”€>â”‚ Spark Streaming  â”‚â”€â”€â”€â”€â”€â”€>â”‚   Gold     â”‚
  â”‚ Events  â”‚        â”‚  (continuous)    â”‚       â”‚  Storage   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘
                              â”‚ trigger & monitor
                              â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚    AIRFLOW      â”‚
                     â”‚  (orchestrator) â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ schedule & coordinate
                              â†“
  BATCH LAYER
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Bronze  â”‚â”€â”€â”€â”€â”€â”€â”€>â”‚   Spark Batch    â”‚â”€â”€â”€â”€â”€â”€>â”‚   Silver   â”‚
  â”‚ Raw Dataâ”‚        â”‚ (hourly/daily)   â”‚       â”‚  Cleaned   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                     AIRFLOW ORCHESTRATES:
                     â€¢ When batches run
                     â€¢ Data validation
                     â€¢ Retry logic
                     â€¢ Monitoring & alerts
```

**One sentence to remember**:

> **Kafka moves data, Spark thinks about data, Airflow decides _when_ things happen.**

---

## ðŸŽ¯ Key Takeaways

1. **DAG = Blueprint** â†’ Defines workflow logic
2. **Scheduler = Brain** â†’ Decides when to run
3. **Executor = Hands** â†’ Runs tasks (local/distributed)
4. **Metadata DB = Memory** â†’ Tracks every execution
5. **UI = Eyes** â†’ Visibility into pipeline health
6. **XCom = Messenger** â†’ Pass data between tasks
7. **Operators = Templates** â†’ Reusable task patterns

**Airflow never replaces Kafka or Spark. It coordinates them.**
